{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy, BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pandas as pd\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------ Custom Environment Wrapper ------------------\n",
    "\n",
    "class CustomHighwayEnv(gym.Wrapper):\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info,_ = self.env.step(action)\n",
    "        if done:\n",
    "            reward = +1000\n",
    "        else:\n",
    "            reward = -50\n",
    "        return obs, reward, done, info,_  \n",
    "\n",
    "# ------------------ Behavior Cloning ------------------\n",
    "\n",
    "class BehaviorCloningPolicy:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = keras.models.load_model(model_path)\n",
    "\n",
    "    def predict(self, ego_vehicle, vehicles):\n",
    "        obs = [self.vehicle_to_observation(ego_vehicle)]\n",
    "        for vehicle in vehicles:\n",
    "            obs.append(self.vehicle_to_observation(vehicle))\n",
    "        while len(obs) < 5:\n",
    "            obs.append([0, 0, 0, 0, 0])  # Padding\n",
    "        obs = np.array(obs).reshape(1, 5, 5)\n",
    "        action_probs = self.model.predict(obs)\n",
    "        return np.argmax(action_probs[0])\n",
    "\n",
    "    @staticmethod\n",
    "    def vehicle_to_observation(vehicle):\n",
    "        return [\n",
    "            1.0,\n",
    "            vehicle.position[0] / 100.0,\n",
    "            vehicle.position[1] / 5.0,\n",
    "            vehicle.speed / 30.0,\n",
    "            vehicle.heading / (2 * np.pi)\n",
    "        ]\n",
    "\n",
    "# ------------------ Custom Network and Policy ------------------\n",
    "\n",
    "class D2RLNetwork(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim: int = 256):\n",
    "        super(D2RLNetwork, self).__init__(observation_space, features_dim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space.shape), features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.d2rl1 = nn.Sequential(\n",
    "            nn.Linear(features_dim, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.d2rl2 = nn.Sequential(\n",
    "            nn.Linear(features_dim, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        x = self.flatten(observations)\n",
    "        x1 = self.hidden(x)\n",
    "        x2 = self.d2rl1(x1)\n",
    "        x3 = self.d2rl2(x1 + x2)\n",
    "        return x3\n",
    "\n",
    "class D2RLPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(D2RLPolicy, self).__init__(*args, **kwargs, features_extractor_class=D2RLNetwork, features_extractor_kwargs=dict(features_dim=256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./tensorboard_a2c/A2C_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=100, episode_reward=630.00 +/- 215.87\n",
      "Episode length: 8.40 +/- 4.32\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 8.4       |\n",
      "|    mean_reward        | 630       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 100       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.54     |\n",
      "|    explained_variance | -6.06e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19        |\n",
      "|    policy_loss        | 556       |\n",
      "|    value_loss         | 3.89e+05  |\n",
      "-------------------------------------\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=200, episode_reward=620.00 +/- 222.71\n",
      "Episode length: 8.60 +/- 4.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.6      |\n",
      "|    mean_reward        | 620      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 200      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 2.58e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39       |\n",
      "|    policy_loss        | 1.31e+03 |\n",
      "|    value_loss         | 7.44e+05 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=300, episode_reward=830.00 +/- 102.96\n",
      "Episode length: 4.40 +/- 2.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.4      |\n",
      "|    mean_reward        | 830      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 300      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 2.2e-05  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59       |\n",
      "|    policy_loss        | 808      |\n",
      "|    value_loss         | 5.42e+05 |\n",
      "------------------------------------\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=400, episode_reward=690.00 +/- 222.26\n",
      "Episode length: 7.20 +/- 4.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.2      |\n",
      "|    mean_reward        | 690      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 400      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 6.14e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79       |\n",
      "|    policy_loss        | 1.1e+03  |\n",
      "|    value_loss         | 6.76e+05 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=240.00 +/- 308.87\n",
      "Episode length: 16.20 +/- 6.18\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 16.2      |\n",
      "|    mean_reward        | 240       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.53     |\n",
      "|    explained_variance | -2.03e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 1e+03     |\n",
      "|    value_loss         | 6.76e+05  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.04     |\n",
      "|    ep_rew_mean     | 598      |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=600, episode_reward=280.00 +/- 455.63\n",
      "Episode length: 15.40 +/- 9.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 15.4     |\n",
      "|    mean_reward        | 280      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 600      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 9.78e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119      |\n",
      "|    policy_loss        | 77       |\n",
      "|    value_loss         | 2.17e+05 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=700, episode_reward=140.00 +/- 356.93\n",
      "Episode length: 18.20 +/- 7.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 18.2     |\n",
      "|    mean_reward        | 140      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 700      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 3.74e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 139      |\n",
      "|    policy_loss        | -189     |\n",
      "|    value_loss         | 2.66e+04 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=800, episode_reward=420.00 +/- 338.53\n",
      "Episode length: 12.60 +/- 6.77\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 12.6      |\n",
      "|    mean_reward        | 420       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 800       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.41     |\n",
      "|    explained_variance | -4.29e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 159       |\n",
      "|    policy_loss        | 4.5       |\n",
      "|    value_loss         | 2.17e+05  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=900, episode_reward=660.00 +/- 124.10\n",
      "Episode length: 7.80 +/- 2.48\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 7.8       |\n",
      "|    mean_reward        | 660       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 900       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | -1.07e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 179       |\n",
      "|    policy_loss        | 679       |\n",
      "|    value_loss         | 5.38e+05  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=620.00 +/- 102.96\n",
      "Episode length: 8.60 +/- 2.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.6      |\n",
      "|    mean_reward        | 620      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 6.32e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 739      |\n",
      "|    value_loss         | 5.32e+05 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.2      |\n",
      "|    ep_rew_mean     | 690      |\n",
      "| time/              |          |\n",
      "|    fps             | 31       |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Training and testing complete!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create environment\n",
    "    base_env = gym.make(\"highway-fast-v0\")\n",
    "    env = CustomHighwayEnv(gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\"))\n",
    "\n",
    "    # Load the pretrained behavior cloning model\n",
    "    model_path = \"C:\\\\Users\\\\Ram\\\\highway\\\\perturbed_models\\\\perturbed_model_1.keras\"\n",
    "    bc_policy = BehaviorCloningPolicy(model_path)\n",
    "\n",
    "    # Callbacks for evaluation during training and tensorboard\n",
    "    eval_callback = EvalCallback(env, best_model_save_path='./logs_a2c/best_model',\n",
    "                                 log_path='./logs_a2c/results', eval_freq=100, n_eval_episodes=5)\n",
    "\n",
    "    # Train A2C with the custom D2RL policy\n",
    "    model = A2C(D2RLPolicy, env, verbose=1, tensorboard_log=\"./tensorboard_a2c/\")\n",
    "    model.learn(total_timesteps=1000, callback=eval_callback)\n",
    "\n",
    "    # Test the trained model and save rewards to CSV\n",
    "    model = A2C.load(\"C:\\\\Users\\\\Ram\\\\highway\\\\logs_a2c\\\\best_model\\\\best_model.zip\", env=env)\n",
    "\n",
    "    episode_data = {}\n",
    "    for ep in range(10000):\n",
    "        obs = env.reset()\n",
    "        if len(obs) > 1:\n",
    "            obs = obs[0]\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _,_ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            env.render()\n",
    "        episode_data[ep+1] = episode_reward\n",
    "\n",
    "    # Save episode rewards to CSV\n",
    "    pd.DataFrame({\"Episode \": list(episode_data.keys()), \"Reward\": list(episode_data.values())}).to_csv(\"d2rl_A2C.csv\", index=False)\n",
    "    print(\"Training and testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "highwayenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
