{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy, BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "# ------------------ Custom Environment Wrapper ------------------\n",
    "\n",
    "class CustomHighwayEnv(gym.Wrapper):\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info,_ = self.env.step(action)\n",
    "        if done:\n",
    "            reward = +1000\n",
    "        else:\n",
    "            reward = -50\n",
    "        return obs, reward, done, info,_  \n",
    "\n",
    "# ------------------ Behavior Cloning ------------------\n",
    "\n",
    "class BehaviorCloningPolicy:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = keras.models.load_model(model_path)\n",
    "\n",
    "    def predict(self, ego_vehicle, vehicles):\n",
    "        obs = [self.vehicle_to_observation(ego_vehicle)]\n",
    "        for vehicle in vehicles:\n",
    "            obs.append(self.vehicle_to_observation(vehicle))\n",
    "        while len(obs) < 5:\n",
    "            obs.append([0, 0, 0, 0, 0])  # Padding\n",
    "        obs = np.array(obs).reshape(1, 5, 5)\n",
    "        action_probs = self.model.predict(obs)\n",
    "        return np.argmax(action_probs[0])\n",
    "\n",
    "    @staticmethod\n",
    "    def vehicle_to_observation(vehicle):\n",
    "        return [\n",
    "            1.0,\n",
    "            vehicle.position[0] / 100.0,\n",
    "            vehicle.position[1] / 5.0,\n",
    "            vehicle.speed / 30.0,\n",
    "            vehicle.heading / (2 * np.pi)\n",
    "        ]\n",
    "\n",
    "# ------------------ Custom Network and Policy ------------------\n",
    "\n",
    "class D2RLNetwork(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim: int = 256):\n",
    "        super(D2RLNetwork, self).__init__(observation_space, features_dim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space.shape), features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.d2rl1 = nn.Sequential(\n",
    "            nn.Linear(features_dim, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.d2rl2 = nn.Sequential(\n",
    "            nn.Linear(features_dim, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        x = self.flatten(observations)\n",
    "        x1 = self.hidden(x)\n",
    "        x2 = self.d2rl1(x1)\n",
    "        x3 = self.d2rl2(x1 + x2)\n",
    "        return x3\n",
    "\n",
    "class D2RLPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(D2RLPolicy, self).__init__(*args, **kwargs, features_extractor_class=D2RLNetwork, features_extractor_kwargs=dict(features_dim=256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./tensorboard/PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=100, episode_reward=630.00 +/- 150.33\n",
      "Episode length: 8.40 +/- 3.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.4      |\n",
      "|    mean_reward     | 630      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100      |\n",
      "---------------------------------\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=200, episode_reward=800.00 +/- 154.92\n",
      "Episode length: 5.00 +/- 3.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 800      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200      |\n",
      "---------------------------------\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=300, episode_reward=810.00 +/- 66.33\n",
      "Episode length: 4.80 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 810      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300      |\n",
      "---------------------------------\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=400, episode_reward=710.00 +/- 146.29\n",
      "Episode length: 6.80 +/- 2.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 710      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=820.00 +/- 97.98\n",
      "Episode length: 4.60 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 820      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=600, episode_reward=680.00 +/- 107.70\n",
      "Episode length: 7.40 +/- 2.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 680      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=700, episode_reward=640.00 +/- 247.79\n",
      "Episode length: 8.20 +/- 4.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.2      |\n",
      "|    mean_reward     | 640      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=800, episode_reward=650.00 +/- 170.29\n",
      "Episode length: 8.00 +/- 3.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | 650      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=900, episode_reward=670.00 +/- 188.68\n",
      "Episode length: 7.60 +/- 3.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 670      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=770.00 +/- 87.18\n",
      "Episode length: 5.60 +/- 1.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 770      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1100, episode_reward=630.00 +/- 116.62\n",
      "Episode length: 8.40 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.4      |\n",
      "|    mean_reward     | 630      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1100     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1200, episode_reward=610.00 +/- 159.37\n",
      "Episode length: 8.80 +/- 3.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | 610      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1200     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1300, episode_reward=760.00 +/- 120.00\n",
      "Episode length: 5.80 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 760      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1300     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1400, episode_reward=330.00 +/- 143.53\n",
      "Episode length: 14.40 +/- 2.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.4     |\n",
      "|    mean_reward     | 330      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1400     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1500, episode_reward=620.00 +/- 166.13\n",
      "Episode length: 8.60 +/- 3.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.6      |\n",
      "|    mean_reward     | 620      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1600, episode_reward=800.00 +/- 63.25\n",
      "Episode length: 5.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 800      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1600     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1700, episode_reward=660.00 +/- 295.63\n",
      "Episode length: 7.80 +/- 5.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | 660      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1700     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1800, episode_reward=730.00 +/- 60.00\n",
      "Episode length: 6.40 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 730      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1800     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1900, episode_reward=400.00 +/- 320.94\n",
      "Episode length: 13.00 +/- 6.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | 400      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1900     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ram\\anaconda3.0\\envs\\highwayenv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=640.00 +/- 159.37\n",
      "Episode length: 8.20 +/- 3.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.2      |\n",
      "|    mean_reward     | 640      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.2     |\n",
      "|    ep_rew_mean     | 456      |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Training and testing complete!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------ Main Execution ------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create environment\n",
    "    base_env = gym.make(\"highway-fast-v0\")\n",
    "    env = CustomHighwayEnv(gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\"))\n",
    "\n",
    "    # Load the pretrained behavior cloning model\n",
    "    model_path = \"C:\\\\Users\\\\Ram\\\\highway\\\\perturbed_models\\\\perturbed_model_1.keras\"\n",
    "    bc_policy = BehaviorCloningPolicy(model_path)\n",
    "\n",
    "    # Callbacks for evaluation during training and tensorboard\n",
    "    eval_callback = EvalCallback(env, best_model_save_path='./logs/best_model',\n",
    "                                 log_path='./logs/results', eval_freq=100, n_eval_episodes=5)\n",
    "\n",
    "    # Train PPO with the custom D2RL policy\n",
    "    model = PPO(D2RLPolicy, env, verbose=1, tensorboard_log=\"./tensorboard/\")\n",
    "    model.learn(total_timesteps=1000, callback=eval_callback)\n",
    "\n",
    "    # Test the trained model and save rewards to CSV\n",
    "    model = PPO.load(\"C:\\\\Users\\\\Ram\\\\highway\\\\logs\\\\best_model\\\\best_model.zip\", env=env)\n",
    "\n",
    "    episode_data = {}\n",
    "    for ep in range(10000):\n",
    "        obs = env.reset()\n",
    "        if len(obs) > 1:\n",
    "            obs = obs[0]\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _,_ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            env.render()\n",
    "        episode_data[ep+1] = episode_reward\n",
    "\n",
    "    # Save episode rewards to CSV\n",
    "    pd.DataFrame({\"Episode \": list(episode_data.keys()), \"Reward\": list(episode_data.values())}).to_csv(\"d2rl_PPO.csv\", index=False)\n",
    "    print(\"Training and testing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "highwayenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
